{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_freelance_nov_2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXPgIfKfbPZJ"
      },
      "source": [
        "# These are the relevant dependencies\n",
        "'''\n",
        "Keras==2.4.3\n",
        "tensorflow==2.3.0\n",
        "Pillow==7.0.0\n",
        "gym==0.17.3\n",
        "h5py==2.10.0\n",
        "numpy==1.18.5\n",
        "pandas==1.1.4\n",
        "scikit-image==0.16.2\n",
        "matplotlib==3.2.2\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLM_c28AXrA3",
        "outputId": "d76df431-4839-4778-88bd-9b68c3b00532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "import gym\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "\n",
        "EPISODES = 100\n",
        "\n",
        "\n",
        "#Custom Environment\n",
        "class Env():\n",
        "\n",
        "    #to get the variable_1 as described in problem\n",
        "    def get_var(self):\n",
        "        return random.randint(0,100) # return random no in the given range\n",
        "\n",
        "    #to get the ground truth as described in problem\n",
        "    def get_ground_truth(self):\n",
        "        return random.randint(90,95) # return random no in the given range\n",
        "\n",
        "    # each iteration step\n",
        "    def step(self, action, i,state):\n",
        "\n",
        "        alpha = self.alpha\n",
        "        reward = 0\n",
        "        gndtrt = self.get_ground_truth()\n",
        "        variable_1 = self.get_var()\n",
        "        if action == 0:\n",
        "            alpha = alpha - 0.05\n",
        "            if(i>1):\n",
        "                state[i] = alpha*state[i-1] + (1-alpha)*(variable_1)         # if action is 0, decrease alpha\n",
        "            else:\n",
        "                state[i] = alpha + (1-alpha)*(variable_1)\n",
        "            if(state[i] > gndtrt):\n",
        "                return 100\n",
        "            else :\n",
        "                return 0\n",
        "\n",
        "        if action == 1:\n",
        "            alpha = alpha + 0.05\n",
        "            if(i>1):\n",
        "                state[i] = alpha*state[i-1] + (1-alpha)*(variable_1)         # if action is 1, increase alpha \n",
        "            else:\n",
        "                state[i] = alpha + (1-alpha)*(variable_1)\n",
        "            if(state[i] > gndtrt) :\n",
        "                return 100\n",
        "            else :\n",
        "                return 0\n",
        "\n",
        "    \n",
        "    # to reset the variables after epochs\n",
        "    def reset(self,state):\n",
        "\n",
        "        self.alpha = 0\n",
        "        state[0] = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Double DQN Agent for the Cartpole\n",
        "# it uses Neural Network to approximate q function\n",
        "# and replay memory & target q network\n",
        "class DoubleDQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "\n",
        "        self.load_model = False\n",
        "        # get size of state and action\n",
        "        self.state_size = 20 #state_size\n",
        "        self.action_size = 2 #action_size\n",
        "\n",
        "        # these is hyper parameters for the Double DQN\n",
        "        self.discount_factor = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.epsilon_min = 0.01\n",
        "        self.batch_size = 64\n",
        "        self.train_start = 1000\n",
        "        # create replay memory using deque\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        # create main model and target model\n",
        "        self.model = self.build_model()\n",
        "        self.target_model = self.build_model()\n",
        "\n",
        "        # initialize target model\n",
        "        self.update_target_model()\n",
        "\n",
        "        #if self.load_model:\n",
        "          #  self.model.load_weights(\"./save_model/var1.h5\")\n",
        "\n",
        "    # approximate Q function using Neural Network\n",
        "    # state is input and Q Value of each action is output of network\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(24, activation='relu',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(self.action_size, activation='linear',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        model.summary()\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    # after some time interval update the target model to be same with model\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    # get action from model using epsilon-greedy policy\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        else:\n",
        "            \n",
        "            state = np.reshape(state, [1, state_size])\n",
        "\n",
        "            q_value = self.model.predict(state)\n",
        "            return np.argmax(q_value[0])\n",
        "\n",
        "    # save sample <s,a,r,s'> to the replay memory\n",
        "    def append_sample(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    # pick samples randomly from replay memory (with batch_size)\n",
        "    def train_model(self):\n",
        "        if len(self.memory) < self.train_start:\n",
        "            return\n",
        "        batch_size = min(self.batch_size, len(self.memory))\n",
        "        mini_batch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        update_input = np.zeros((batch_size, self.state_size))\n",
        "        update_target = np.zeros((batch_size, self.state_size))\n",
        "        action, reward, done = [], [], []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            update_input[i] = mini_batch[i][0]\n",
        "            action.append(mini_batch[i][1])\n",
        "            reward.append(mini_batch[i][2])\n",
        "            update_target[i] = mini_batch[i][3]\n",
        "            done.append(mini_batch[i][4])\n",
        "\n",
        "\n",
        "        target = self.model.predict(update_input)\n",
        "        target_next = self.model.predict(update_target)\n",
        "        target_val = self.target_model.predict(update_target)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # like Q Learning, get maximum Q value at s'\n",
        "            # But from target model\n",
        "            if done[i]:\n",
        "                target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                # the key point of Double DQN\n",
        "                # selection of action is from model\n",
        "                # update is from target model\n",
        "                a = np.argmax(target_next[i])\n",
        "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
        "                    target_val[i][a])\n",
        "\n",
        "        # make minibatch which includes target q value and predicted q value\n",
        "        # and do the model fit!\n",
        "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
        "                       epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "      \n",
        "    env = Env()\n",
        "    # get size of state and action from environment\n",
        "    state_size = 20 \n",
        "    action_size = 2 \n",
        "\n",
        "    #creates state: size 20 as that was the number of iterations mentioned in problem statement\n",
        "    state = [0]*20\n",
        "\n",
        "    agent = DoubleDQNAgent(state_size, action_size)\n",
        "\n",
        "    #these lists store the final score of each epoch\n",
        "    scores, episodes = [], []\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        done = False\n",
        "        score = 0\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        #This is to reset the states after each epoch\n",
        "        state = [0]*20\n",
        "        env.reset(state)\n",
        "        while not done:\n",
        "            i = i+1\n",
        "            if(i==19):\n",
        "                done = True\n",
        "\n",
        "            # get action for the current state and go one step in environment\n",
        "            action = agent.get_action(state)\n",
        "\n",
        "            reward = env.step(action,i,state)\n",
        "            # if an action make the episode end, then gives penalty of -100\n",
        "            reward = reward if not done else 0\n",
        "\n",
        "            # save the sample <s, a, r, s'> to the replay memory\n",
        "            if(i<19):\n",
        "                agent.append_sample(state[i], action, reward, state[i+1], done)\n",
        "            else:\n",
        "                agent.append_sample(state[i], action, reward, 0, done)\n",
        "\n",
        "            # every time step do the training\n",
        "            agent.train_model()\n",
        "            score = score + reward\n",
        "\n",
        "            if done:\n",
        "                # every episode update the target model to be same with model\n",
        "                agent.update_target_model()\n",
        "\n",
        "                # every episode, plot the play time\n",
        "                scores.append(score)\n",
        "                episodes.append(e)\n",
        "                \n",
        "                # TO PLOT A GRAPH OF EPISODES VS SCORES\n",
        "                #pylab.plot(episodes, scores, 'b')\n",
        "                #pylab.savefig(\"episodes_vs_scores.png\")\n",
        "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
        "\n",
        "           \n",
        "\n",
        "        # to save the model after 50 epochs each time\n",
        "        if e % 50 == 0:\n",
        "            agent.model.save_weights(\"var1.h5\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 24)                504       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 50        \n",
            "=================================================================\n",
            "Total params: 1,154\n",
            "Trainable params: 1,154\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 24)                504       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 50        \n",
            "=================================================================\n",
            "Total params: 1,154\n",
            "Trainable params: 1,154\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "episode: 0   score: 100   memory length: 19   epsilon: 0.9811700348643991\n",
            "episode: 1   score: 0   memory length: 38   epsilon: 0.9626946373158061\n",
            "episode: 2   score: 200   memory length: 57   epsilon: 0.9445671308589195\n",
            "episode: 3   score: 0   memory length: 76   epsilon: 0.9267809647166116\n",
            "episode: 4   score: 0   memory length: 95   epsilon: 0.9093297114626595\n",
            "episode: 5   score: 400   memory length: 114   epsilon: 0.8922070646990518\n",
            "episode: 6   score: 0   memory length: 133   epsilon: 0.8754068367770318\n",
            "episode: 7   score: 200   memory length: 152   epsilon: 0.8589229565610536\n",
            "episode: 8   score: 300   memory length: 171   epsilon: 0.8427494672348417\n",
            "episode: 9   score: 0   memory length: 190   epsilon: 0.8268805241487632\n",
            "episode: 10   score: 0   memory length: 209   epsilon: 0.8113103927077344\n",
            "episode: 11   score: 200   memory length: 228   epsilon: 0.7960334462988972\n",
            "episode: 12   score: 200   memory length: 247   epsilon: 0.7810441642583167\n",
            "episode: 13   score: 0   memory length: 266   epsilon: 0.766337129875968\n",
            "episode: 14   score: 200   memory length: 285   epsilon: 0.7519070284382872\n",
            "episode: 15   score: 0   memory length: 304   epsilon: 0.7377486453075807\n",
            "episode: 16   score: 200   memory length: 323   epsilon: 0.723856864037602\n",
            "episode: 17   score: 0   memory length: 342   epsilon: 0.7102266645246085\n",
            "episode: 18   score: 100   memory length: 361   epsilon: 0.6968531211932361\n",
            "episode: 19   score: 0   memory length: 380   epsilon: 0.6837314012165328\n",
            "episode: 20   score: 400   memory length: 399   epsilon: 0.6708567627695098\n",
            "episode: 21   score: 0   memory length: 418   epsilon: 0.6582245533155777\n",
            "episode: 22   score: 200   memory length: 437   epsilon: 0.6458302079252489\n",
            "episode: 23   score: 300   memory length: 456   epsilon: 0.6336692476264985\n",
            "episode: 24   score: 0   memory length: 475   epsilon: 0.6217372777861888\n",
            "episode: 25   score: 100   memory length: 494   epsilon: 0.6100299865219717\n",
            "episode: 26   score: 0   memory length: 513   epsilon: 0.598543143144092\n",
            "episode: 27   score: 100   memory length: 532   epsilon: 0.5872725966265356\n",
            "episode: 28   score: 0   memory length: 551   epsilon: 0.576214274106964\n",
            "episode: 29   score: 100   memory length: 570   epsilon: 0.5653641794148941\n",
            "episode: 30   score: 400   memory length: 589   epsilon: 0.5547183916275941\n",
            "episode: 31   score: 300   memory length: 608   epsilon: 0.54427306365317\n",
            "episode: 32   score: 200   memory length: 627   epsilon: 0.534024420840334\n",
            "episode: 33   score: 0   memory length: 646   epsilon: 0.5239687596143511\n",
            "episode: 34   score: 400   memory length: 665   epsilon: 0.5141024461386688\n",
            "episode: 35   score: 200   memory length: 684   epsilon: 0.5044219150017507\n",
            "episode: 36   score: 100   memory length: 703   epsilon: 0.49492366792863446\n",
            "episode: 37   score: 400   memory length: 722   epsilon: 0.48560427251675453\n",
            "episode: 38   score: 200   memory length: 741   epsilon: 0.47646036099556505\n",
            "episode: 39   score: 200   memory length: 760   epsilon: 0.46748862900952265\n",
            "episode: 40   score: 0   memory length: 779   epsilon: 0.4586858344239834\n",
            "episode: 41   score: 200   memory length: 798   epsilon: 0.45004879615358584\n",
            "episode: 42   score: 100   memory length: 817   epsilon: 0.44157439301269474\n",
            "episode: 43   score: 200   memory length: 836   epsilon: 0.43325956258749154\n",
            "episode: 44   score: 300   memory length: 855   epsilon: 0.4251013001293033\n",
            "episode: 45   score: 100   memory length: 874   epsilon: 0.41709665746876984\n",
            "episode: 46   score: 400   memory length: 893   epsilon: 0.40924274195045723\n",
            "episode: 47   score: 100   memory length: 912   epsilon: 0.4015367153875324\n",
            "episode: 48   score: 100   memory length: 931   epsilon: 0.3939757930361214\n",
            "episode: 49   score: 500   memory length: 950   epsilon: 0.3865572425889805\n",
            "episode: 50   score: 200   memory length: 969   epsilon: 0.379278383188116\n",
            "episode: 51   score: 100   memory length: 988   epsilon: 0.37213658445599673\n",
            "episode: 52   score: 300   memory length: 1007   epsilon: 0.36512926554500874\n",
            "episode: 53   score: 100   memory length: 1026   epsilon: 0.35825389420480874\n",
            "episode: 54   score: 200   memory length: 1045   epsilon: 0.35150798586723914\n",
            "episode: 55   score: 0   memory length: 1064   epsilon: 0.34488910274847373\n",
            "episode: 56   score: 100   memory length: 1083   epsilon: 0.33839485296807126\n",
            "episode: 57   score: 200   memory length: 1102   epsilon: 0.33202288968461574\n",
            "episode: 58   score: 200   memory length: 1121   epsilon: 0.325770910247633\n",
            "episode: 59   score: 0   memory length: 1140   epsilon: 0.31963665536547703\n",
            "episode: 60   score: 300   memory length: 1159   epsilon: 0.31361790828888503\n",
            "episode: 61   score: 100   memory length: 1178   epsilon: 0.3077124940099052\n",
            "episode: 62   score: 100   memory length: 1197   epsilon: 0.30191827847590974\n",
            "episode: 63   score: 300   memory length: 1216   epsilon: 0.29623316781840775\n",
            "episode: 64   score: 0   memory length: 1235   epsilon: 0.2906551075963786\n",
            "episode: 65   score: 200   memory length: 1254   epsilon: 0.2851820820538545\n",
            "episode: 66   score: 100   memory length: 1273   epsilon: 0.2798121133914823\n",
            "episode: 67   score: 100   memory length: 1292   epsilon: 0.27454326105180193\n",
            "episode: 68   score: 100   memory length: 1311   epsilon: 0.26937362101798235\n",
            "episode: 69   score: 300   memory length: 1330   epsilon: 0.2643013251257632\n",
            "episode: 70   score: 100   memory length: 1349   epsilon: 0.259324540388352\n",
            "episode: 71   score: 0   memory length: 1368   epsilon: 0.2544414683340336\n",
            "episode: 72   score: 100   memory length: 1387   epsilon: 0.24965034435625258\n",
            "episode: 73   score: 300   memory length: 1406   epsilon: 0.24494943707593364\n",
            "episode: 74   score: 100   memory length: 1425   epsilon: 0.24033704771580874\n",
            "episode: 75   score: 300   memory length: 1444   epsilon: 0.2358115094865268\n",
            "episode: 76   score: 100   memory length: 1463   epsilon: 0.23137118698432196\n",
            "episode: 77   score: 100   memory length: 1482   epsilon: 0.22701447560002463\n",
            "episode: 78   score: 200   memory length: 1501   epsilon: 0.22273980093919937\n",
            "episode: 79   score: 0   memory length: 1520   epsilon: 0.21854561825320348\n",
            "episode: 80   score: 200   memory length: 1539   epsilon: 0.21443041188095732\n",
            "episode: 81   score: 100   memory length: 1558   epsilon: 0.21039269470122635\n",
            "episode: 82   score: 0   memory length: 1577   epsilon: 0.20643100759521713\n",
            "episode: 83   score: 200   memory length: 1596   epsilon: 0.20254391891929227\n",
            "episode: 84   score: 0   memory length: 1615   epsilon: 0.19873002398761402\n",
            "episode: 85   score: 400   memory length: 1634   epsilon: 0.19498794456453009\n",
            "episode: 86   score: 200   memory length: 1653   epsilon: 0.1913163283665175\n",
            "episode: 87   score: 200   memory length: 1672   epsilon: 0.1877138485735047\n",
            "episode: 88   score: 200   memory length: 1691   epsilon: 0.18417920334939616\n",
            "episode: 89   score: 200   memory length: 1710   epsilon: 0.18071111537162424\n",
            "episode: 90   score: 200   memory length: 1729   epsilon: 0.17730833136956103\n",
            "episode: 91   score: 100   memory length: 1748   epsilon: 0.17396962167162064\n",
            "episode: 92   score: 200   memory length: 1767   epsilon: 0.17069377976089034\n",
            "episode: 93   score: 200   memory length: 1786   epsilon: 0.1674796218391289\n",
            "episode: 94   score: 400   memory length: 1805   epsilon: 0.16432598639897444\n",
            "episode: 95   score: 200   memory length: 1824   epsilon: 0.16123173380420852\n",
            "episode: 96   score: 300   memory length: 1843   epsilon: 0.1581957458779228\n",
            "episode: 97   score: 200   memory length: 1862   epsilon: 0.15521692549844113\n",
            "episode: 98   score: 200   memory length: 1881   epsilon: 0.15229419620285028\n",
            "episode: 99   score: 300   memory length: 1900   epsilon: 0.14942650179799613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGrz-Kb3ZHum",
        "outputId": "ae79db73-88f2-4dd7-82a7-da04582db4d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# I have mentioned the relevant dependencies above, but still if you need anytign else here are the other ones\n",
        "!pip freeze"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "absl-py==0.10.0\n",
            "alabaster==0.7.12\n",
            "albumentations==0.1.12\n",
            "altair==4.1.0\n",
            "argon2-cffi==20.1.0\n",
            "asgiref==3.3.0\n",
            "astor==0.8.1\n",
            "astropy==4.1\n",
            "astunparse==1.6.3\n",
            "async-generator==1.10\n",
            "atari-py==0.2.6\n",
            "atomicwrites==1.4.0\n",
            "attrs==20.2.0\n",
            "audioread==2.1.9\n",
            "autograd==1.3\n",
            "Babel==2.8.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.6.3\n",
            "bleach==3.2.1\n",
            "blis==0.4.1\n",
            "bokeh==2.1.1\n",
            "Bottleneck==1.3.2\n",
            "branca==0.4.1\n",
            "bs4==0.0.1\n",
            "CacheControl==0.12.6\n",
            "cachetools==4.1.1\n",
            "catalogue==1.0.0\n",
            "certifi==2020.6.20\n",
            "cffi==1.14.3\n",
            "chainer==7.4.0\n",
            "chardet==3.0.4\n",
            "click==7.1.2\n",
            "cloudpickle==1.3.0\n",
            "cmake==3.12.0\n",
            "cmdstanpy==0.9.5\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "contextlib2==0.5.5\n",
            "convertdate==2.2.2\n",
            "coverage==3.7.1\n",
            "coveralls==0.5\n",
            "crcmod==1.7\n",
            "cufflinks==0.17.3\n",
            "cvxopt==1.2.5\n",
            "cvxpy==1.0.31\n",
            "cycler==0.10.0\n",
            "cymem==2.0.4\n",
            "Cython==0.29.21\n",
            "daft==0.0.4\n",
            "dask==2.12.0\n",
            "dataclasses==0.7\n",
            "datascience==0.10.6\n",
            "debugpy==1.0.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.6.0\n",
            "descartes==1.1.0\n",
            "dill==0.3.3\n",
            "distributed==1.25.3\n",
            "Django==3.1.3\n",
            "dlib==19.18.0\n",
            "dm-tree==0.1.5\n",
            "docopt==0.6.2\n",
            "docutils==0.16\n",
            "dopamine-rl==1.0.5\n",
            "earthengine-api==0.1.238\n",
            "easydict==1.9\n",
            "ecos==2.0.7.post1\n",
            "editdistance==0.5.3\n",
            "en-core-web-sm==2.2.5\n",
            "entrypoints==0.3\n",
            "ephem==3.7.7.1\n",
            "et-xmlfile==1.0.1\n",
            "fa2==0.3.5\n",
            "fancyimpute==0.4.3\n",
            "fastai==1.0.61\n",
            "fastdtw==0.3.4\n",
            "fastprogress==1.0.0\n",
            "fastrlock==0.5\n",
            "fbprophet==0.7.1\n",
            "feather-format==0.4.1\n",
            "filelock==3.0.12\n",
            "firebase-admin==4.4.0\n",
            "fix-yahoo-finance==0.0.22\n",
            "Flask==1.1.2\n",
            "folium==0.8.3\n",
            "future==0.16.0\n",
            "gast==0.3.3\n",
            "GDAL==2.2.2\n",
            "gdown==3.6.4\n",
            "gensim==3.6.0\n",
            "geographiclib==1.50\n",
            "geopy==1.17.0\n",
            "gin-config==0.3.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==1.16.0\n",
            "google-api-python-client==1.7.12\n",
            "google-auth==1.17.2\n",
            "google-auth-httplib2==0.0.4\n",
            "google-auth-oauthlib==0.4.2\n",
            "google-cloud-bigquery==1.21.0\n",
            "google-cloud-bigquery-storage==1.1.0\n",
            "google-cloud-core==1.0.3\n",
            "google-cloud-datastore==1.8.0\n",
            "google-cloud-firestore==1.7.0\n",
            "google-cloud-language==1.2.0\n",
            "google-cloud-storage==1.18.1\n",
            "google-cloud-translate==1.5.0\n",
            "google-colab==1.0.0\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==0.4.1\n",
            "googleapis-common-protos==1.52.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.10.1\n",
            "grpcio==1.33.2\n",
            "gspread==3.0.1\n",
            "gspread-dataframe==3.0.8\n",
            "gym==0.17.3\n",
            "h5py==2.10.0\n",
            "HeapDict==1.0.1\n",
            "holidays==0.10.3\n",
            "holoviews==1.13.5\n",
            "html5lib==1.0.1\n",
            "httpimport==0.5.18\n",
            "httplib2==0.17.4\n",
            "httplib2shim==0.0.3\n",
            "humanize==0.5.1\n",
            "hyperopt==0.1.2\n",
            "ideep4py==2.0.0.post3\n",
            "idna==2.10\n",
            "image==1.5.33\n",
            "imageio==2.4.1\n",
            "imagesize==1.2.0\n",
            "imbalanced-learn==0.4.3\n",
            "imblearn==0.0\n",
            "imgaug==0.2.9\n",
            "importlib-metadata==2.0.0\n",
            "importlib-resources==3.3.0\n",
            "imutils==0.5.3\n",
            "inflect==2.1.0\n",
            "iniconfig==1.1.1\n",
            "intel-openmp==2020.0.133\n",
            "intervaltree==2.1.0\n",
            "ipykernel==4.10.1\n",
            "ipython==5.5.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.3.9\n",
            "ipywidgets==7.5.1\n",
            "itsdangerous==1.1.0\n",
            "jax==0.2.4\n",
            "jaxlib==0.1.56+cuda101\n",
            "jdcal==1.4.1\n",
            "jedi==0.17.2\n",
            "jieba==0.42.1\n",
            "Jinja2==2.11.2\n",
            "joblib==0.17.0\n",
            "jpeg4py==0.1.4\n",
            "jsonschema==2.6.0\n",
            "jupyter==1.0.0\n",
            "jupyter-client==5.3.5\n",
            "jupyter-console==5.2.0\n",
            "jupyter-core==4.6.3\n",
            "jupyterlab-pygments==0.1.2\n",
            "kaggle==1.5.9\n",
            "kapre==0.1.3.1\n",
            "Keras==2.4.3\n",
            "Keras-Preprocessing==1.1.2\n",
            "keras-vis==0.4.1\n",
            "kiwisolver==1.3.1\n",
            "knnimpute==0.1.0\n",
            "korean-lunar-calendar==0.2.1\n",
            "librosa==0.6.3\n",
            "lightgbm==2.2.3\n",
            "llvmlite==0.31.0\n",
            "lmdb==0.99\n",
            "lucid==0.3.8\n",
            "LunarCalendar==0.0.9\n",
            "lxml==4.2.6\n",
            "Markdown==3.3.3\n",
            "MarkupSafe==1.1.1\n",
            "matplotlib==3.2.2\n",
            "matplotlib-venn==0.11.6\n",
            "missingno==0.4.2\n",
            "mistune==0.8.4\n",
            "mizani==0.6.0\n",
            "mkl==2019.0\n",
            "mlxtend==0.14.0\n",
            "more-itertools==8.6.0\n",
            "moviepy==0.2.3.5\n",
            "mpmath==1.1.0\n",
            "msgpack==1.0.0\n",
            "multiprocess==0.70.10\n",
            "multitasking==0.0.9\n",
            "murmurhash==1.0.3\n",
            "music21==5.5.0\n",
            "natsort==5.5.0\n",
            "nbclient==0.5.1\n",
            "nbconvert==5.6.1\n",
            "nbformat==5.0.8\n",
            "nest-asyncio==1.4.2\n",
            "networkx==2.5\n",
            "nibabel==3.0.2\n",
            "nltk==3.2.5\n",
            "notebook==5.3.1\n",
            "np-utils==0.5.12.1\n",
            "numba==0.48.0\n",
            "numexpr==2.7.1\n",
            "numpy==1.18.5\n",
            "nvidia-ml-py3==7.352.0\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.1.0\n",
            "okgrade==0.4.3\n",
            "opencv-contrib-python==4.1.2.30\n",
            "opencv-python==4.1.2.30\n",
            "openpyxl==2.5.9\n",
            "opt-einsum==3.3.0\n",
            "osqp==0.6.1\n",
            "packaging==20.4\n",
            "palettable==3.3.0\n",
            "pandas==1.1.4\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.13.3\n",
            "pandas-profiling==1.4.1\n",
            "pandocfilters==1.4.3\n",
            "panel==0.9.7\n",
            "param==1.10.0\n",
            "parso==0.7.1\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.1\n",
            "pexpect==4.8.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==7.0.0\n",
            "pip-tools==4.5.1\n",
            "plac==1.1.3\n",
            "plotly==4.4.1\n",
            "plotnine==0.6.0\n",
            "pluggy==0.7.1\n",
            "portpicker==1.3.1\n",
            "prefetch-generator==1.0.1\n",
            "preshed==3.0.2\n",
            "prettytable==1.0.1\n",
            "progressbar2==3.38.0\n",
            "prometheus-client==0.8.0\n",
            "promise==2.3\n",
            "prompt-toolkit==1.0.18\n",
            "protobuf==3.12.4\n",
            "psutil==5.4.8\n",
            "psycopg2==2.7.6.1\n",
            "ptyprocess==0.6.0\n",
            "py==1.9.0\n",
            "pyarrow==0.14.1\n",
            "pyasn1==0.4.8\n",
            "pyasn1-modules==0.2.8\n",
            "pycocotools==2.0.2\n",
            "pycparser==2.20\n",
            "pyct==0.4.8\n",
            "pydata-google-auth==1.1.0\n",
            "pydot==1.3.0\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "pyemd==0.5.1\n",
            "pyglet==1.5.0\n",
            "Pygments==2.6.1\n",
            "pygobject==3.26.1\n",
            "pymc3==3.7\n",
            "PyMeeus==0.3.7\n",
            "pymongo==3.11.0\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.5\n",
            "pyparsing==2.4.7\n",
            "pyrsistent==0.17.3\n",
            "pysndfile==1.3.8\n",
            "PySocks==1.7.1\n",
            "pystan==2.19.1.1\n",
            "pytest==3.6.4\n",
            "python-apt==1.6.5+ubuntu0.3\n",
            "python-chess==0.23.11\n",
            "python-dateutil==2.8.1\n",
            "python-louvain==0.14\n",
            "python-slugify==4.0.1\n",
            "python-utils==2.4.0\n",
            "pytz==2018.9\n",
            "pyviz-comms==0.7.6\n",
            "PyWavelets==1.1.1\n",
            "PyYAML==3.13\n",
            "pyzmq==19.0.2\n",
            "qtconsole==4.7.7\n",
            "QtPy==1.9.0\n",
            "regex==2019.12.20\n",
            "requests==2.23.0\n",
            "requests-oauthlib==1.3.0\n",
            "resampy==0.2.2\n",
            "retrying==1.3.3\n",
            "rpy2==3.2.7\n",
            "rsa==4.6\n",
            "scikit-image==0.16.2\n",
            "scikit-learn==0.22.2.post1\n",
            "scipy==1.4.1\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==2.1.2\n",
            "seaborn==0.11.0\n",
            "Send2Trash==1.5.0\n",
            "setuptools-git==1.2\n",
            "Shapely==1.7.1\n",
            "simplegeneric==0.8.1\n",
            "six==1.15.0\n",
            "sklearn==0.0\n",
            "sklearn-pandas==1.8.0\n",
            "slugify==0.0.1\n",
            "smart-open==3.0.0\n",
            "snowballstemmer==2.0.0\n",
            "sortedcontainers==2.2.2\n",
            "spacy==2.2.4\n",
            "Sphinx==1.8.5\n",
            "sphinxcontrib-serializinghtml==1.1.4\n",
            "sphinxcontrib-websupport==1.2.4\n",
            "SQLAlchemy==1.3.20\n",
            "sqlparse==0.4.1\n",
            "srsly==1.0.2\n",
            "statsmodels==0.10.2\n",
            "sympy==1.1.1\n",
            "tables==3.4.4\n",
            "tabulate==0.8.7\n",
            "tblib==1.7.0\n",
            "tensorboard==2.3.0\n",
            "tensorboard-plugin-wit==1.7.0\n",
            "tensorboardcolab==0.0.22\n",
            "tensorflow==2.3.0\n",
            "tensorflow-addons==0.8.3\n",
            "tensorflow-datasets==4.0.1\n",
            "tensorflow-estimator==2.3.0\n",
            "tensorflow-gcs-config==2.3.0\n",
            "tensorflow-hub==0.10.0\n",
            "tensorflow-metadata==0.24.0\n",
            "tensorflow-privacy==0.2.2\n",
            "tensorflow-probability==0.11.0\n",
            "termcolor==1.1.0\n",
            "terminado==0.9.1\n",
            "testpath==0.4.4\n",
            "text-unidecode==1.3\n",
            "textblob==0.15.3\n",
            "textgenrnn==1.4.1\n",
            "Theano==1.0.5\n",
            "thinc==7.4.0\n",
            "tifffile==2020.9.3\n",
            "toml==0.10.2\n",
            "toolz==0.11.1\n",
            "torch==1.7.0+cu101\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.3.1\n",
            "torchvision==0.8.1+cu101\n",
            "tornado==5.1.1\n",
            "tqdm==4.41.1\n",
            "traitlets==4.3.3\n",
            "tweepy==3.6.0\n",
            "typeguard==2.7.1\n",
            "typing-extensions==3.7.4.3\n",
            "tzlocal==1.5.1\n",
            "umap-learn==0.4.6\n",
            "uritemplate==3.0.1\n",
            "urllib3==1.24.3\n",
            "vega-datasets==0.8.0\n",
            "wasabi==0.8.0\n",
            "wcwidth==0.2.5\n",
            "webencodings==0.5.1\n",
            "Werkzeug==1.0.1\n",
            "widgetsnbextension==3.5.1\n",
            "wordcloud==1.5.0\n",
            "wrapt==1.12.1\n",
            "xarray==0.15.1\n",
            "xgboost==0.90\n",
            "xkit==0.0.0\n",
            "xlrd==1.1.0\n",
            "xlwt==1.3.0\n",
            "yellowbrick==0.9.1\n",
            "zict==2.0.0\n",
            "zipp==3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uke3jbcbaM_t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}